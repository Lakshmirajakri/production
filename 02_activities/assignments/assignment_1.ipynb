{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with parquet files\n",
    "\n",
    "## Objective\n",
    "\n",
    "+ In this assignment, we will use the data downloaded with the module `data_manager` to create features.\n",
    "\n",
    "(11 pts total)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "+ This notebook assumes that price data is available to you in the environment variable `PRICE_DATA`. If you have not done so, then execute the notebook `01_materials/labs/2_data_engineering.ipynb` to create this data set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Load the environment variables using dotenv. (1 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Load the environment variable `PRICE_DATA`.\n",
    "+ Use [glob](https://docs.python.org/3/library/glob.html) to find the path of all parquet files in the directory `PRICE_DATA`.\n",
    "\n",
    "(1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.getenv('SRC_DIR'))\n",
    "from utils.logger import get_logger\n",
    "_logs = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 01:26:20,375, 2330488740.py, 3, INFO, Loaded PRICE_DATA: ../../05_src/data/prices/\n"
     ]
    }
   ],
   "source": [
    "# Load the environment variable PRICE_DATA\n",
    "PRICE_DATA = os.getenv('PRICE_DATA')\n",
    "_logs.info(f\"Loaded PRICE_DATA: {PRICE_DATA}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2725 parquet files\n"
     ]
    }
   ],
   "source": [
    "# Using glob to find the path of all parquet files\n",
    "parquet_files = glob(os.path.join(PRICE_DATA, \"**\", \"*.parquet\"), recursive=True)\n",
    "print(f\"Found {len(parquet_files)} parquet files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 01:27:26,994, 4029740748.py, 2, INFO, First 5 parquet files: ['../../05_src/data/prices/BKTI/BKTI_2012/part.0.parquet', '../../05_src/data/prices/BKTI/BKTI_2012/part.1.parquet', '../../05_src/data/prices/BKTI/BKTI_2015/part.0.parquet', '../../05_src/data/prices/BKTI/BKTI_2015/part.1.parquet', '../../05_src/data/prices/BKTI/BKTI_2014/part.0.parquet']\n"
     ]
    }
   ],
   "source": [
    "# Verification\n",
    "_logs.info(f\"First 5 parquet files: {parquet_files[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each ticker and using Dask, do the following:\n",
    "\n",
    "+ Add lags for variables Close and Adj_Close.\n",
    "+ Add returns based on Close:\n",
    "    \n",
    "    - `returns`: (Close / Close_lag_1) - 1\n",
    "\n",
    "+ Add the following range: \n",
    "\n",
    "    - `hi_lo_range`: this is the day's High minus Low.\n",
    "\n",
    "+ Assign the result to `dd_feat`.\n",
    "\n",
    "(4 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 01:29:39,900, 2114134884.py, 4, INFO, Loaded parquet files into Dask Dataframe (dd_px).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "dd_px dtypes: Date          datetime64[ns]\n",
      "Open                 float64\n",
      "High                 float64\n",
      "Low                  float64\n",
      "Close                float64\n",
      "Adj Close            float64\n",
      "Volume               float64\n",
      "source       string[pyarrow]\n",
      "ticker       string[pyarrow]\n",
      "Year                   int32\n",
      "dtype: object\n",
      "Columns in the dataset: ['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'source', 'ticker', 'Year', 'Close_lag_1', 'Adj_Close_lag_1', 'returns', 'hi_lo_range']\n"
     ]
    }
   ],
   "source": [
    "# DO NOT set ticker as index - keep default index\n",
    "dd_px = dd.read_parquet(parquet_files)\n",
    "\n",
    "_logs.info(\"Loaded parquet files into Dask Dataframe (dd_px).\")\n",
    "print(\"\\n\")\n",
    "print(f\"dd_px dtypes: {dd_px.dtypes}\")\n",
    "\n",
    "# Columns\n",
    "columns = ddf.columns.tolist()\n",
    "print(f\"Columns in the dataset: {columns}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(group):\n",
    "    \"\"\"Function to add features to each ticker group\"\"\"\n",
    "    # Sort by date to ensure proper lag calculations\n",
    "    group = group.sort_values('Date')\n",
    "    \n",
    "    # Add lag feature\n",
    "    group = group.assign(\n",
    "        Close_lag_1=group['Close'].shift(1),\n",
    "        Adj_Close_lag_1=group['Adj Close'].shift(1)\n",
    "    )\n",
    "    \n",
    "    # Add returns\n",
    "    group = group.assign(\n",
    "        returns=(group['Close'] / group['Close_lag_1']) - 1\n",
    "    )\n",
    "    \n",
    "    # Add hi_lo range\n",
    "    group = group.assign(\n",
    "        hi_lo_range=group['High'] - group['Low']\n",
    "    )\n",
    "    \n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 01:30:49,273, 3577835805.py, 2, INFO, Creating meta specification\n"
     ]
    }
   ],
   "source": [
    "# Get a small sample to create proper meta specification\n",
    "_logs.info(\"Creating meta specification\")\n",
    "sample_data = dd_px.head(100)\n",
    "sample_ticker = sample_data['ticker'].iloc[0]\n",
    "sample_group = sample_data[sample_data['ticker'] == sample_ticker]\n",
    "sample_with_features = add_features(sample_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 01:31:04,379, 2825510153.py, 3, INFO, Meta specification created with 14 columns\n"
     ]
    }
   ],
   "source": [
    "# Create meta dictionary based on sample\n",
    "meta_dict = {col: sample_with_features[col].dtype for col in sample_with_features.columns}\n",
    "_logs.info(f\"Meta specification created with {len(meta_dict)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 01:31:44,819, 645997329.py, 2, INFO, Applying feature engineering to all ticker groups\n",
      "2025-06-11 01:31:44,824, 645997329.py, 8, INFO, Successfully added all features:\n",
      "2025-06-11 01:31:44,825, 645997329.py, 9, INFO, - Close_lag_1: Previous day's closing price\n",
      "2025-06-11 01:31:44,826, 645997329.py, 10, INFO, - Adj_Close_lag_1: Previous day's adjusted closing price\n",
      "2025-06-11 01:31:44,827, 645997329.py, 11, INFO, - returns: (Close / Close_lag_1) - 1\n",
      "2025-06-11 01:31:44,827, 645997329.py, 12, INFO, - hi_lo_range: High - Low\n",
      "2025-06-11 01:31:44,828, 645997329.py, 14, INFO, dd_feat columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'source', 'ticker', 'Year', 'Close_lag_1', 'Adj_Close_lag_1', 'returns', 'hi_lo_range']\n"
     ]
    }
   ],
   "source": [
    "# Apply group function to the whole data - no need to reset_index since we didn't set one\n",
    "_logs.info(\"Applying feature engineering to all ticker groups\")\n",
    "dd_feat = dd_px.groupby('ticker', group_keys=False).apply(\n",
    "    add_features,\n",
    "    meta=meta_dict\n",
    ")\n",
    "\n",
    "_logs.info(\"Successfully added all features:\")\n",
    "_logs.info(\"- Close_lag_1: Previous day's closing price\")\n",
    "_logs.info(\"- Adj_Close_lag_1: Previous day's adjusted closing price\")\n",
    "_logs.info(\"- returns: (Close / Close_lag_1) - 1\")\n",
    "_logs.info(\"- hi_lo_range: High - Low\")\n",
    "\n",
    "_logs.info(f\"dd_feat columns: {dd_feat.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 01:32:23,610, 4243127812.py, 2, INFO, Converting Dask dataframe to pandas dataframe\n",
      "2025-06-11 01:32:40,992, 4243127812.py, 4, INFO, Converted to pandas dataframe with shape: (311001, 14)\n"
     ]
    }
   ],
   "source": [
    "# Convert to pandas\n",
    "_logs.info(\"Converting Dask dataframe to pandas dataframe\")\n",
    "df_feat = dd_feat.compute()\n",
    "_logs.info(f\"Converted to pandas dataframe with shape: {df_feat.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 01:32:58,318, 1889558176.py, 2, INFO, Adding 10-day moving average of returns\n",
      "2025-06-11 01:32:58,430, 1889558176.py, 12, INFO, Added 'returns_ma_10' feature (10-day moving average of returns).\n",
      "2025-06-11 01:32:58,430, 1889558176.py, 13, INFO, Final dataframe shape: (311001, 15)\n"
     ]
    }
   ],
   "source": [
    "# Adding moving average of returns with 10-day window\n",
    "_logs.info(\"Adding 10-day moving average of returns\")\n",
    "\n",
    "# Sort ticker and date to ensure proper order for rolling calculations\n",
    "df_feat = df_feat.sort_values(['ticker', 'Date'])\n",
    "\n",
    "# Calculate 10 day moving average of returns for each ticker\n",
    "df_feat['returns_ma_10'] = df_feat.groupby('ticker')['returns'].transform(\n",
    "    lambda x: x.rolling(10).mean()\n",
    ")\n",
    "\n",
    "_logs.info(\"Added 'returns_ma_10' feature (10-day moving average of returns).\")\n",
    "_logs.info(f\"Final dataframe shape: {df_feat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 01:33:13,178, 3463744243.py, 6, INFO, Final columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'source', 'ticker', 'Year', 'Close_lag_1', 'Adj_Close_lag_1', 'returns', 'hi_lo_range', 'returns_ma_10']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assignment completed successfully!\n",
      "Final dataframe shape: (311001, 15)\n",
      "Number of columns: 15\n"
     ]
    }
   ],
   "source": [
    "# Simple verification - just show basic info\n",
    "print(\"Assignment completed successfully!\")\n",
    "print(f\"Final dataframe shape: {df_feat.shape}\")\n",
    "print(f\"Number of columns: {len(df_feat.columns)}\")\n",
    "\n",
    "_logs.info(f\"Final columns: {df_feat.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 01:33:22,513, 4125431718.py, 4, INFO, Sample of returns_ma_10 by ticker:\n",
      "       ticker       Date     Close  returns  returns_ma_10\n",
      "38749       A 1999-12-03   31.8312   0.0085         0.0024\n",
      "38750       A 1999-12-06   32.7253   0.0281         0.0135\n",
      "38751       A 1999-12-07   32.3677  -0.0109         0.0034\n",
      "131198    ACB 2000-01-18   39.0312  -0.0068        -0.0045\n",
      "131199    ACB 2000-01-19   39.0312   0.0000        -0.0045\n",
      "131200    ACB 2000-01-20   38.7657  -0.0068        -0.0020\n",
      "2618     ALDX 2014-05-16    7.2200  -0.0296         0.0036\n",
      "2619     ALDX 2014-05-19    7.4100   0.0263         0.0132\n",
      "2620     ALDX 2014-05-20    7.3400  -0.0094         0.0212\n",
      "206167   AMAT 1980-03-31    0.0946   0.1122         0.0015\n",
      "206168   AMAT 1980-04-01    0.0938  -0.0092         0.0024\n",
      "206169   AMAT 1980-04-02    0.0885  -0.0556        -0.0050\n",
      "36750    AMPY 2012-05-04  144.6000  -0.0062        -0.0032\n",
      "36751    AMPY 2012-05-07  142.2000  -0.0166        -0.0089\n",
      "36752    AMPY 2012-05-08  142.0000  -0.0014        -0.0079\n"
     ]
    }
   ],
   "source": [
    "# Select a nice sample that shows the moving average in action\n",
    "sample_view = df_feat[df_feat['returns_ma_10'].notna()].groupby('ticker').head(3)\n",
    "sample_table = sample_view[['ticker', 'Date', 'Close', 'returns', 'returns_ma_10']].round(4)\n",
    "_logs.info(f\"Sample of returns_ma_10 by ticker:\\n{sample_table.head(15).to_string()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Convert the Dask data frame to a pandas data frame. \n",
    "+ Add a new feature containing the moving average of `returns` using a window of 10 days. There are several ways to solve this task, a simple one uses `.rolling(10).mean()`.\n",
    "\n",
    "(3 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please comment:\n",
    "\n",
    "+ Was it necessary to convert to pandas to calculate the moving average return?\n",
    "+ Would it have been better to do it in Dask? Why?\n",
    "\n",
    "(1 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was not strictly necessary to convert to pandas. Dask supports rolling window operations like .rolling().mean() starting from version 2022.10.0, but with some limitations â€” for example, it requires a known partition structure and often works best when the data is sorted and indexed correctly. In this case, converting to pandas made the rolling calculation more straightforward and allowed immediate inspection of results.\n",
    "\n",
    "However, for large-scale datasets that don't fit into memory, it would be better to keep the data in Dask and use Dask's rolling methods combined with proper partitioning and indexing for scalability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criteria\n",
    "\n",
    "The [rubric](./assignment_1_rubric_clean.xlsx) contains the criteria for grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Information\n",
    "\n",
    "ðŸš¨ **Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)** ðŸš¨ for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "\n",
    "### Submission Parameters:\n",
    "* Submission Due Date: `HH:MM AM/PM - DD/MM/YYYY`\n",
    "* The branch name for your repo should be: `assignment-1`\n",
    "* What to submit for this assignment:\n",
    "    * This Jupyter Notebook (assignment_1.ipynb) should be populated and should be the only change in your pull request.\n",
    "* What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/production/pull/<pr_id>`\n",
    "    * Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "\n",
    "Checklist:\n",
    "- [ ] Created a branch with the correct naming convention.\n",
    "- [ ] Ensured that the repository is public.\n",
    "- [ ] Reviewed the PR description guidelines and adhered to them.\n",
    "- [ ] Verify that the link is accessible in a private browser window.\n",
    "\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack at `#cohort-3-help`. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsi_participant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
